[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rww.science/blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nOregon County Support for Retaining Slavery in the OR Constitution\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nRWW\n\n\n\n\n\n\n  \n\n\n\n\nMarveling at map()\n\n\nCelebrating map\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15 Characters that Made Me Better in the Classroom\n\n\nCelebrating quarto, reveal.js and multiplex\n\n\n\n\nR\n\n\nhybrid\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2022\n\n\nRWW\n\n\n\n\n\n\n  \n\n\n\n\nA Pigeon Becomes a Barplot\n\n\n\n\n\n\n\nggplot\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlows to Hidalgo County\n\n\n\n\n\n\n\nMaps\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Voters\n\n\n\n\n\n\n\nMaps\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlows to Marion County\n\n\n\n\n\n\n\nggplot\n\n\nMaps\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInflation Expectations\n\n\n\n\n\n\n\ndataviz\n\n\nggplot\n\n\ntime series\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2022\n\n\nRWW\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNational Weather Service Portland, Part II\n\n\n\n\n\n\n\nggplot\n\n\ntidyverse\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2022\n\n\nRWW\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2022-11-09-slavery-OR/index.html",
    "href": "posts/2022-11-09-slavery-OR/index.html",
    "title": "Oregon County Support for Retaining Slavery in the OR Constitution",
    "section": "",
    "text": "Last update: November 15. 2022\nIn preparation for the dumpster fire that is Oregon election reporting, I previously posed on importing a directory of .csv files. At present, that is what I can find to build this. What does the interface look like?\nThis is terrible, there is a javascript button to download each separately. Nevertheless, here we go.\nFirst, to import the various files. I am going to use an import then export trick to make this easier. First, let me use the directory to create the county names.\nWith that I can pull in each file, add the county name to it, and save it back.\nNow to use these to create the data.\nWhat does it look like?"
  },
  {
    "objectID": "posts/2022-11-09-slavery-OR/index.html#peeling-the-results-of-interest",
    "href": "posts/2022-11-09-slavery-OR/index.html#peeling-the-results-of-interest",
    "title": "Oregon County Support for Retaining Slavery in the OR Constitution",
    "section": "Peeling the results of interest",
    "text": "Peeling the results of interest\n\nSlavery.Res <- Oregon.County.Results %>%\n  filter(ContestID==100002574 & CandidateName==\"No\") %>%\n  select(County, CandidatePercentage)\nlibrary(tigris); library(rgdal); library(htmltools); library(viridis); library(sf); library(ggrepel)\ncounties.t <- counties(state = \"41\", resolution = \"500k\", class=\"sf\")\nMap.Me <- left_join(counties.t,Slavery.Res, by=c(\"NAME\" = \"County\"))\n\nNow to map it.\n\nMy.Map <- Map.Me %>% \n  ggplot(., aes(geometry=geometry, fill=CandidatePercentage, label=NAME, group=NAME)) + \n  geom_sf() +\n  geom_label_repel(stat = \"sf_coordinates\",\n    min.segment.length = 0,\n    colour = \"white\",\n    segment.colour = \"white\",\n    size = 1,\n    box.padding = unit(0.05, \"lines\")) +\n  scale_fill_continuous_tableau(\"Red\") + \n  theme_minimal() + \n  labs(title=\"Remove Slavery Measure from Oregon Constitution\", \n       x=\"\", \n       y=\"\", \n       fill=\"Percentage No's\")\n\nHere is the map.\n\nMy.Map"
  },
  {
    "objectID": "posts/2022-11-09-slavery-OR/index.html#a-regression",
    "href": "posts/2022-11-09-slavery-OR/index.html#a-regression",
    "title": "Oregon County Support for Retaining Slavery in the OR Constitution",
    "section": "A Regression",
    "text": "A Regression\nI want to estimate a simple regression on some of these data; how much of the variance in No votes for removing slavery from the Oregon Constitution can be explained by support for Christine Drazan.\n\nOregon.County.Results %>% \n  filter((ContestID==100002574 & CandidateName==\"No\") | CandidateName==\"Christine Drazan\") %>% \n  select(County, CandidatePercentage, CandidateName) %>%\n  pivot_wider(., names_from=\"CandidateName\", values_from=\"CandidatePercentage\") %>% \n  lm(`No` ~ `Christine Drazan`, data=.) %>% summary\n\n\nCall:\nlm(formula = No ~ `Christine Drazan`, data = .)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.037100 -0.020341 -0.005199  0.011769  0.074723 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.08045    0.01956   4.114 0.000233 ***\n`Christine Drazan`  0.88101    0.03261  27.020  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03019 on 34 degrees of freedom\nMultiple R-squared:  0.9555,    Adjusted R-squared:  0.9542 \nF-statistic: 730.1 on 1 and 34 DF,  p-value: < 2.2e-16\n\n\nWhoa! Almost 96% using the current totals as of 10AM on the day after the election.\n\nlibrary(emoGG)\nOregon.County.Results %>% \n  filter((ContestID==100002574 & CandidateName==\"No\") | CandidateName==\"Christine Drazan\") %>% \n  select(County, CandidatePercentage, CandidateName) %>%\n  pivot_wider(., names_from=\"CandidateName\", values_from=\"CandidatePercentage\") %>% \n  ggplot() + \n  aes(x=`Christine Drazan`, y=No) + \n  geom_point(color=\"purple\") + \n  geom_smooth(method=\"lm\") +\n  theme_minimal() +\n  labs(y=\"No to Removing Slavery\")\n\n\n\n\n\nlibrary(plotly)\nOregon.County.Results %>% \n  filter((ContestID==100002574 & CandidateName==\"No\") | CandidateName==\"Christine Drazan\") %>% \n  select(County, CandidatePercentage, CandidateName) %>%\n  pivot_wider(., names_from=\"CandidateName\", values_from=\"CandidatePercentage\") %>% \n  ggplot() + \n  aes(x=`Christine Drazan`, y=No, label=County) + \n  geom_point() + \n  geom_smooth(method=\"lm\") + theme_minimal() +\n  labs(y=\"Proportion No Votes on Slavery Removal\", x=\"Drazan Vote Proportion\", title=\"Removing Slavery for Convicts from Oregon's Constitution and Drazan Vote\", subtitle=\"By Oregon County, Correlation: 0.9775\") -> pgg\npgg\n\n\n\nggplot2::ggsave(filename = \"image.jpg\", plot=pgg)\n\nAs a plotly\n\nggplotly(pgg)"
  },
  {
    "objectID": "posts/2022-11-04-marveling-at-map/index.html",
    "href": "posts/2022-11-04-marveling-at-map/index.html",
    "title": "Marveling at map()",
    "section": "",
    "text": "I want to learn about map and begin to replace my use of apply for everything.\nGoal: import a directory full of csv files.\nMethod. First, load purrr and the tidyverse and then create a tibble for the filenames because we are creating the example. map wants to output a list so I have to unlist it into the tibble."
  },
  {
    "objectID": "posts/2022-11-04-marveling-at-map/index.html#create-the-files",
    "href": "posts/2022-11-04-marveling-at-map/index.html#create-the-files",
    "title": "Marveling at map()",
    "section": "Create the files",
    "text": "Create the files\nNow I need to create some files in a given directory. I have created a directory called fakedata.\ndir.create(\"fakedata\")\nNow I want to write the data to the directory to conclude the reproducible example. In this case, there are five csv files.\nwalk(fnames$filenames, \n     ~ write.csv(data.frame(x1=rnorm(10),x2=rnorm(10)),       file = paste0(\"./fakedata/\",.x, sep=\"\")))\nThat seems to work.\n\n\n\nPhoto of success"
  },
  {
    "objectID": "posts/2022-11-04-marveling-at-map/index.html#loading-the-files",
    "href": "posts/2022-11-04-marveling-at-map/index.html#loading-the-files",
    "title": "Marveling at map()",
    "section": "Loading the files",
    "text": "Loading the files\nThough I already know the names of the files, in most cases, I would need to collect them. In this particular case, dir() will come in very handy.\n\ndir(\"fakedata\")\n\n[1] \"file1.csv\" \"file2.csv\" \"file3.csv\" \"file4.csv\" \"file5.csv\"\n\nfile.names <- dir(\"fakedata\")\n\nNow let’s take those and load the files. We will combine read.csv with map to get a list containing all of the files.\n\nread.files <- file.names %>% map(., ~ read.csv(paste0(\"fakedata/\",.x, sep=\"\")))\nread.files\n\n[[1]]\n    X          x1          x2\n1   1 -1.38001285 -1.22636540\n2   2  0.18200309 -0.47330201\n3   3  0.38672920  0.32228012\n4   4  1.59951536  0.17438280\n5   5  0.04066402 -2.17050573\n6   6 -0.25814383  0.35882778\n7   7 -1.09942888  0.05209077\n8   8 -0.96198031 -0.31535964\n9   9 -0.71146188 -0.88743843\n10 10 -0.90768963 -1.64463965\n\n[[2]]\n    X          x1          x2\n1   1 -0.93160098  0.38577793\n2   2 -1.11500843 -2.28599247\n3   3  0.48695836 -0.38091065\n4   4  0.43833164  1.94491090\n5   5  1.38558361  0.29454227\n6   6 -0.08697636  0.14269709\n7   7  1.32351741  0.29570675\n8   8 -0.51831633 -1.07216871\n9   9 -0.51175809  0.01959317\n10 10  0.89500573 -1.03716777\n\n[[3]]\n    X         x1          x2\n1   1 -0.8834481 -0.25006254\n2   2  1.0708784 -1.36573365\n3   3  0.3286340 -0.10929292\n4   4  1.2088226 -0.04355173\n5   5 -0.5257318 -0.47974675\n6   6  3.0484664 -0.32511134\n7   7 -0.2252273  1.23925377\n8   8  1.0458161 -1.18513179\n9   9  0.3243315  1.44062060\n10 10  0.8238747 -1.53231879\n\n[[4]]\n    X         x1          x2\n1   1  0.8460534  0.08150364\n2   2 -0.3220720  1.07239215\n3   3  1.3478005  1.18462705\n4   4  0.9222565  1.16223158\n5   5  1.1380822  1.05890088\n6   6  0.6123547 -1.46789361\n7   7 -0.2974722 -0.37149873\n8   8  1.0321546  0.15357759\n9   9 -0.3445316 -0.95253704\n10 10 -0.1112015 -0.64225964\n\n[[5]]\n    X          x1          x2\n1   1 -1.15093999  0.54168693\n2   2  0.35413293 -0.09396014\n3   3 -1.26696402  0.25582265\n4   4  0.06712394  1.23749986\n5   5  0.53648590  0.37711498\n6   6 -1.29457309  0.23440024\n7   7  0.44966760  0.45751493\n8   8 -0.42755728  0.89952539\n9   9 -0.06584558  0.09520603\n10 10 -0.50046918 -0.16258746\n\n\nLet me stick the filenames on the list elements. This allows any individual one to be addressed in a relatively intuitive way. I show an example by calculating the mean of x1 in the third dataset.\n\nclean.files <- read.files\nnames(clean.files) <- file.names\nclean.files\n\n$file1.csv\n    X          x1          x2\n1   1 -1.38001285 -1.22636540\n2   2  0.18200309 -0.47330201\n3   3  0.38672920  0.32228012\n4   4  1.59951536  0.17438280\n5   5  0.04066402 -2.17050573\n6   6 -0.25814383  0.35882778\n7   7 -1.09942888  0.05209077\n8   8 -0.96198031 -0.31535964\n9   9 -0.71146188 -0.88743843\n10 10 -0.90768963 -1.64463965\n\n$file2.csv\n    X          x1          x2\n1   1 -0.93160098  0.38577793\n2   2 -1.11500843 -2.28599247\n3   3  0.48695836 -0.38091065\n4   4  0.43833164  1.94491090\n5   5  1.38558361  0.29454227\n6   6 -0.08697636  0.14269709\n7   7  1.32351741  0.29570675\n8   8 -0.51831633 -1.07216871\n9   9 -0.51175809  0.01959317\n10 10  0.89500573 -1.03716777\n\n$file3.csv\n    X         x1          x2\n1   1 -0.8834481 -0.25006254\n2   2  1.0708784 -1.36573365\n3   3  0.3286340 -0.10929292\n4   4  1.2088226 -0.04355173\n5   5 -0.5257318 -0.47974675\n6   6  3.0484664 -0.32511134\n7   7 -0.2252273  1.23925377\n8   8  1.0458161 -1.18513179\n9   9  0.3243315  1.44062060\n10 10  0.8238747 -1.53231879\n\n$file4.csv\n    X         x1          x2\n1   1  0.8460534  0.08150364\n2   2 -0.3220720  1.07239215\n3   3  1.3478005  1.18462705\n4   4  0.9222565  1.16223158\n5   5  1.1380822  1.05890088\n6   6  0.6123547 -1.46789361\n7   7 -0.2974722 -0.37149873\n8   8  1.0321546  0.15357759\n9   9 -0.3445316 -0.95253704\n10 10 -0.1112015 -0.64225964\n\n$file5.csv\n    X          x1          x2\n1   1 -1.15093999  0.54168693\n2   2  0.35413293 -0.09396014\n3   3 -1.26696402  0.25582265\n4   4  0.06712394  1.23749986\n5   5  0.53648590  0.37711498\n6   6 -1.29457309  0.23440024\n7   7  0.44966760  0.45751493\n8   8 -0.42755728  0.89952539\n9   9 -0.06584558  0.09520603\n10 10 -0.50046918 -0.16258746\n\nclean.files$file3.csv %>% summarise(mean(x1))\n\n   mean(x1)\n1 0.6216417\n\nmean(read.files[[3]]$x1)\n\n[1] 0.6216417\n\n\nIf every spreadsheet happened to be the same, as these are, then we could also stack them. There is a danger to the renaming because of the way it works with unlist. Sticking with the original file, they are straightforward to stack using map_dfr()\n\nstacked.files <- file.names %>% map_dfr(., ~ read.csv(paste0(\"fakedata/\",.x, sep=\"\")))\nstacked.files\n\n    X          x1          x2\n1   1 -1.38001285 -1.22636540\n2   2  0.18200309 -0.47330201\n3   3  0.38672920  0.32228012\n4   4  1.59951536  0.17438280\n5   5  0.04066402 -2.17050573\n6   6 -0.25814383  0.35882778\n7   7 -1.09942888  0.05209077\n8   8 -0.96198031 -0.31535964\n9   9 -0.71146188 -0.88743843\n10 10 -0.90768963 -1.64463965\n11  1 -0.93160098  0.38577793\n12  2 -1.11500843 -2.28599247\n13  3  0.48695836 -0.38091065\n14  4  0.43833164  1.94491090\n15  5  1.38558361  0.29454227\n16  6 -0.08697636  0.14269709\n17  7  1.32351741  0.29570675\n18  8 -0.51831633 -1.07216871\n19  9 -0.51175809  0.01959317\n20 10  0.89500573 -1.03716777\n21  1 -0.88344806 -0.25006254\n22  2  1.07087838 -1.36573365\n23  3  0.32863403 -0.10929292\n24  4  1.20882258 -0.04355173\n25  5 -0.52573184 -0.47974675\n26  6  3.04846641 -0.32511134\n27  7 -0.22522727  1.23925377\n28  8  1.04581612 -1.18513179\n29  9  0.32433151  1.44062060\n30 10  0.82387468 -1.53231879\n31  1  0.84605339  0.08150364\n32  2 -0.32207204  1.07239215\n33  3  1.34780048  1.18462705\n34  4  0.92225652  1.16223158\n35  5  1.13808219  1.05890088\n36  6  0.61235469 -1.46789361\n37  7 -0.29747222 -0.37149873\n38  8  1.03215455  0.15357759\n39  9 -0.34453156 -0.95253704\n40 10 -0.11120152 -0.64225964\n41  1 -1.15093999  0.54168693\n42  2  0.35413293 -0.09396014\n43  3 -1.26696402  0.25582265\n44  4  0.06712394  1.23749986\n45  5  0.53648590  0.37711498\n46  6 -1.29457309  0.23440024\n47  7  0.44966760  0.45751493\n48  8 -0.42755728  0.89952539\n49  9 -0.06584558  0.09520603\n50 10 -0.50046918 -0.16258746"
  },
  {
    "objectID": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html",
    "href": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html",
    "title": "15 Characters that Made Me Better in the Classroom",
    "section": "",
    "text": "“You forgot to share your screen,” is one of the phrases that I will most closely associate with pandemic-era teaching. But, when deploying code in the classroom, screen sharing is not enough. quarto, reveal.js and multiplex: true have empowered me to be better at my job.\nMany academics faced challenges in the transition to hybrid and fully online education during COVID-19. This post is dedicated to recent developments made easy by RStudio’s quarto project that solves three difficult and interrelated problems that frequently arose, and continue to arise, during the ongoing pandemic and in a hybrid teaching world. In concert, I believe that they have improved my effectiveness in online data science education. What are the the two main problems?\nThere is a third interrelated problem; student access to the code on the screen and this problem is present for both in-person and online learners in the hybrid environment."
  },
  {
    "objectID": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#background",
    "href": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#background",
    "title": "15 Characters that Made Me Better in the Classroom",
    "section": "Background",
    "text": "Background\nI have used Yihui Xie’s excellent xaringan package for slides for a few years now. It really is an amazing tool. The thing that prompted my original foray into using it was the easy ability to share code in slides. But with the transition to Zoom during the pandemic and ubiquitously before the pandemic, presenting slides in a classroom or with screen sharing puts a barrier to copying and pasting the provided code.\nThe barrier for in-person learners is that the code is on the projector screen, not their screen. Zoom’s screen share presents a very similar problem, the code is on their screen, but it is an image rather than copy-pasteable text."
  },
  {
    "objectID": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#the-quarto-solution",
    "href": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#the-quarto-solution",
    "title": "15 Characters that Made Me Better in the Classroom",
    "section": "The Quarto Solution",
    "text": "The Quarto Solution\nWith fifteen magic keystrokes, the barriers to code access disappear. Moreover, they have the magnificent side effect of rendering the initial problem moot. Whether or not one shares the screen, the content of the screen share is available to all learners, in-person or hybrid, with only a browser window open.\nmultiplex: true"
  },
  {
    "objectID": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#the-details",
    "href": "posts/2022-09-22-the-magic-of-multiplex-for-hybrid-teaching/index.html#the-details",
    "title": "15 Characters that Made Me Better in the Classroom",
    "section": "The Details",
    "text": "The Details\nMy goal in the classroom is make the process interactive. To a management school academic with the goal of teaching future managers how to glean insights from data, I need code to both give the computer instructions on how to manipulate and display the data that is the source of those insights and to provide the scaffolding for transferring the deployed methods to other problems. I suspect these twin aims are shared by those in the social and natural sciences, management, geography, public health, and all of the disciplines where data science is not the end but instead informs some application in combination with subject-area expertise.\n\nThe Dark Ages: Powerpoint\nLong ago, my tool of choice was Microsoft’s Powerpoint. Don’t judge. In management schools and in industry, Powerpoint is ubiquitous as presentation software and for white paper creation – decks that violate basically every principle of effective presentation. It is familiar. It does a job. But everything becomes curly-quotes without care and this is a nightmare for writing code. Moreover, the workflows involve a lot of unnecessary labor.\n\n\nRMarkdown and The Middle Ages\nWith RMarkdown, \\(\\LaTeX\\) beamer presentations in .pdf format and various versions of .html slides became possible and this saved me a lot of manual copying and pasting. With xaringan by Yihui Xie, nice formatting boxes for code and the magic of various innovations in .html became possible. I still love xaringan and for presentations where the audience is not actively engaged in recreating output, xaringan is still my favorite. Indeed, if I could figure out how to get xaringan to do what multiplex can, I would still use it.\nThe core limitation, as a presenter, of all of the aforementioned implementations of slides in RMarkdown is that the audience needs to follow along with the presenter to obtain the code and to deploy it. No matter how detailed or careful one is, the slides don’t just move themselves.\n\n\nModernity and Multiplexing\nThe pandemic brought new challenges though they were not dissimilar from old challenges. In the hybrid format or in the classroom with a projector or television, the problem remains. The slides don’t move themselves. Though online learners are far closer to the presentation than are those sitting in the auditorium, there is still a barrier to access. Neither the computer screen nor the projector screen is accessible in the sense that the code is rendered on that screen as an image rather than as manipulable text. With quarto and reveal.js, my job and the experience of my students have been vastly improved."
  },
  {
    "objectID": "posts/2022-03-19-inflation-expectations/index.html",
    "href": "posts/2022-03-19-inflation-expectations/index.html",
    "title": "Inflation Expectations",
    "section": "",
    "text": "The Federal Reserve Bank of New York provides interesting data on inflation expectations. I was first interested in this because it appears as though the survey respondents have consistently overestimated inflation expectations over the last few years. But now, in a time of heightened concern about inflation, it is worthwhile to revisit the data.\n\nlibrary(tidyverse)\nlibrary(lubridate); library(tsibble)\nlibrary(readxl); library(magrittr)\nlibrary(kableExtra)\nurl <- \"https://www.newyorkfed.org/medialibrary/interactives/sce/sce/downloads/data/frbny-sce-data.xlsx\"\ndestfile <- \"frbny_sce_data.xlsx\"\ncurl::curl_download(url, destfile)\nInflation.Expectations <- read_excel(destfile, sheet=4, skip=3) \nInflation.Expectations %>% \n  kable() %>%\n  kable_styling() %>%\n  scroll_box(width = \"100%\", height = \"500px\")\n\n\n\n \n  \n    ...1 \n    Median one-year ahead expected inflation rate \n    Median three-year ahead expected inflation rate \n    25th Percentile one-year ahead expected inflation rate \n    25th Percentile three-year ahead expected inflation rate \n    75th Percentile one-year ahead expected inflation rate \n    75th Percentile three-year ahead expected inflation rate \n    Median point prediction one-year ahead inflation rate \n    Median point prediction three-year ahead inflation rate \n  \n \n\n  \n    201306 \n    3.090884 \n    3.416846 \n    1.9766899 \n    1.9509043 \n    5.944291 \n    6.000000 \n    4.569018 \n    4.735714 \n  \n  \n    201307 \n    3.162522 \n    3.310545 \n    1.6002527 \n    1.5759639 \n    6.000000 \n    6.330960 \n    4.682268 \n    4.893152 \n  \n  \n    201308 \n    3.395071 \n    3.799491 \n    2.0000000 \n    1.6044991 \n    7.192735 \n    7.192735 \n    4.800452 \n    5.040974 \n  \n  \n    201309 \n    3.367290 \n    3.546918 \n    1.9176887 \n    1.7006738 \n    6.874382 \n    7.027530 \n    4.754334 \n    4.878838 \n  \n  \n    201310 \n    3.174733 \n    3.196597 \n    1.6670417 \n    1.6044991 \n    6.414917 \n    6.558024 \n    4.619435 \n    4.875060 \n  \n  \n    201311 \n    3.196597 \n    3.314703 \n    1.6856116 \n    1.7418048 \n    6.232919 \n    6.754294 \n    4.087263 \n    4.684939 \n  \n  \n    201312 \n    3.137840 \n    3.299735 \n    1.6044991 \n    1.5653559 \n    6.026756 \n    6.357209 \n    4.518580 \n    4.648178 \n  \n  \n    201401 \n    3.000000 \n    3.046670 \n    1.4001945 \n    1.2561138 \n    6.061110 \n    6.025575 \n    4.123138 \n    4.110649 \n  \n  \n    201402 \n    3.090884 \n    3.177027 \n    1.7688211 \n    1.5469180 \n    6.000000 \n    6.035755 \n    4.131488 \n    4.079364 \n  \n  \n    201403 \n    3.196597 \n    3.377147 \n    1.5469180 \n    1.5469180 \n    6.097920 \n    6.223626 \n    4.521639 \n    4.651865 \n  \n  \n    201404 \n    3.303940 \n    3.204834 \n    1.7923245 \n    1.6874917 \n    6.303314 \n    6.235626 \n    4.473255 \n    4.317511 \n  \n  \n    201405 \n    3.171748 \n    3.196597 \n    1.6044991 \n    1.5469180 \n    6.000000 \n    6.486081 \n    4.342987 \n    4.525708 \n  \n  \n    201406 \n    3.204563 \n    3.297345 \n    1.7576319 \n    1.5000967 \n    6.000000 \n    6.414917 \n    4.752476 \n    4.599026 \n  \n  \n    201407 \n    3.111111 \n    3.218860 \n    1.6995459 \n    1.6775503 \n    6.366208 \n    6.510431 \n    4.539547 \n    4.516861 \n  \n  \n    201408 \n    3.000000 \n    3.085294 \n    1.5752116 \n    1.6321160 \n    6.000000 \n    6.000000 \n    4.039658 \n    4.273867 \n  \n  \n    201409 \n    3.000000 \n    3.015459 \n    1.5734774 \n    1.4624749 \n    6.000000 \n    6.339693 \n    3.873158 \n    4.153529 \n  \n  \n    201410 \n    3.000000 \n    3.015459 \n    1.4624749 \n    1.3130764 \n    6.000000 \n    6.000000 \n    3.458891 \n    3.997284 \n  \n  \n    201411 \n    3.015459 \n    3.034222 \n    1.4624749 \n    1.3726228 \n    6.000000 \n    6.000000 \n    3.608507 \n    4.091155 \n  \n  \n    201412 \n    2.980844 \n    3.000000 \n    1.0000000 \n    1.1738002 \n    5.650182 \n    5.869063 \n    3.084015 \n    4.022125 \n  \n  \n    201501 \n    2.926264 \n    3.000000 \n    1.1019423 \n    1.2069269 \n    5.423985 \n    5.985323 \n    3.315294 \n    4.147226 \n  \n  \n    201502 \n    2.830265 \n    3.000000 \n    1.1975554 \n    1.1976033 \n    5.162554 \n    5.904663 \n    3.180199 \n    4.131799 \n  \n  \n    201503 \n    2.875452 \n    2.915113 \n    1.0760911 \n    1.0156784 \n    4.950111 \n    5.487096 \n    3.120108 \n    3.616236 \n  \n  \n    201504 \n    2.733259 \n    2.980844 \n    1.0000000 \n    1.0000000 \n    4.997518 \n    5.643198 \n    3.157371 \n    3.888054 \n  \n  \n    201505 \n    2.955763 \n    2.980844 \n    1.2880070 \n    1.2215939 \n    4.906164 \n    5.337633 \n    3.257119 \n    3.802553 \n  \n  \n    201506 \n    3.000000 \n    3.000000 \n    1.4025799 \n    1.2880070 \n    5.760430 \n    5.593090 \n    3.469145 \n    3.786062 \n  \n  \n    201507 \n    2.955357 \n    2.955680 \n    1.2880070 \n    1.1221224 \n    5.734184 \n    5.570208 \n    3.182219 \n    3.741563 \n  \n  \n    201508 \n    2.793575 \n    2.867507 \n    1.0000000 \n    1.1019423 \n    5.191454 \n    5.530786 \n    3.065382 \n    3.714436 \n  \n  \n    201509 \n    2.734659 \n    2.842578 \n    1.0432696 \n    1.0000000 \n    4.708744 \n    5.423985 \n    3.158668 \n    3.259574 \n  \n  \n    201510 \n    2.823326 \n    2.781905 \n    1.0252963 \n    1.0000000 \n    5.190876 \n    5.165283 \n    3.063085 \n    3.153144 \n  \n  \n    201511 \n    2.554007 \n    2.681402 \n    1.0000000 \n    1.0000000 \n    4.733266 \n    5.045190 \n    3.049218 \n    3.144082 \n  \n  \n    201512 \n    2.537525 \n    2.775065 \n    1.0000000 \n    1.0000000 \n    4.730198 \n    5.075049 \n    3.097423 \n    3.130671 \n  \n  \n    201601 \n    2.417507 \n    2.453082 \n    1.0000000 \n    0.8968862 \n    4.712423 \n    4.590201 \n    2.954850 \n    3.043420 \n  \n  \n    201602 \n    2.711993 \n    2.622853 \n    1.0000000 \n    1.0000000 \n    5.143975 \n    5.054150 \n    3.107146 \n    3.387503 \n  \n  \n    201603 \n    2.530770 \n    2.500490 \n    1.0000000 \n    1.0000000 \n    4.488211 \n    4.708021 \n    3.089970 \n    3.104698 \n  \n  \n    201604 \n    2.613696 \n    2.791493 \n    1.0621943 \n    1.0000000 \n    5.084910 \n    5.487096 \n    3.089232 \n    3.153507 \n  \n  \n    201605 \n    2.622853 \n    2.734135 \n    1.0000000 \n    1.0000000 \n    4.997918 \n    5.240709 \n    3.144871 \n    3.097219 \n  \n  \n    201606 \n    2.537871 \n    2.861800 \n    1.0000000 \n    1.0000000 \n    4.894264 \n    5.309123 \n    3.144044 \n    3.652668 \n  \n  \n    201607 \n    2.521351 \n    2.501432 \n    1.0000000 \n    1.0000000 \n    5.083651 \n    5.272973 \n    3.231861 \n    3.443616 \n  \n  \n    201608 \n    2.793575 \n    2.711993 \n    1.0000000 \n    0.9995852 \n    5.283558 \n    5.559269 \n    3.098649 \n    3.424709 \n  \n  \n    201609 \n    2.495162 \n    2.623311 \n    1.0000000 \n    1.0000000 \n    4.812058 \n    5.242156 \n    3.044933 \n    3.551025 \n  \n  \n    201610 \n    2.588671 \n    2.582562 \n    1.0000000 \n    1.0000000 \n    5.111414 \n    5.501368 \n    3.075901 \n    3.217843 \n  \n  \n    201611 \n    2.537525 \n    2.711993 \n    1.0000000 \n    0.9995852 \n    4.747997 \n    5.337633 \n    3.114850 \n    3.099744 \n  \n  \n    201612 \n    2.812191 \n    2.831908 \n    1.0000000 \n    1.0000000 \n    5.191454 \n    5.576227 \n    3.171784 \n    3.822607 \n  \n  \n    201701 \n    2.980844 \n    2.895548 \n    1.0000000 \n    1.0000000 \n    5.075049 \n    5.552731 \n    3.131943 \n    3.819542 \n  \n  \n    201702 \n    2.961960 \n    2.980844 \n    1.0000000 \n    1.0000000 \n    5.309342 \n    5.640829 \n    3.134774 \n    3.688152 \n  \n  \n    201703 \n    2.736074 \n    2.711993 \n    1.0000000 \n    1.0000000 \n    4.707835 \n    5.423985 \n    3.099358 \n    3.099730 \n  \n  \n    201704 \n    2.793575 \n    2.908013 \n    1.0110189 \n    1.0000000 \n    4.852659 \n    5.600195 \n    3.028089 \n    3.177438 \n  \n  \n    201705 \n    2.588671 \n    2.467132 \n    1.0000000 \n    0.9603176 \n    4.416696 \n    5.084910 \n    3.070494 \n    3.077746 \n  \n  \n    201706 \n    2.538351 \n    2.775065 \n    1.0000000 \n    1.0000000 \n    4.000000 \n    4.830334 \n    2.993377 \n    3.499542 \n  \n  \n    201707 \n    2.537525 \n    2.711993 \n    1.0000000 \n    1.0000000 \n    4.143766 \n    4.912187 \n    3.001880 \n    3.177373 \n  \n  \n    201708 \n    2.490645 \n    2.623311 \n    1.0262393 \n    1.0000000 \n    4.000000 \n    4.636709 \n    2.964123 \n    3.033085 \n  \n  \n    201709 \n    2.537525 \n    2.804799 \n    1.0760593 \n    1.0000000 \n    4.353259 \n    4.955311 \n    3.002994 \n    3.101659 \n  \n  \n    201710 \n    2.613696 \n    2.812191 \n    1.0275226 \n    1.0000000 \n    4.734380 \n    4.906164 \n    3.021131 \n    3.035650 \n  \n  \n    201711 \n    2.606111 \n    2.775065 \n    1.0000000 \n    1.0000000 \n    4.076428 \n    5.123124 \n    3.015359 \n    3.354707 \n  \n  \n    201712 \n    2.824348 \n    2.894165 \n    1.0139530 \n    1.0489619 \n    4.812058 \n    5.423985 \n    3.090315 \n    3.383863 \n  \n  \n    201801 \n    2.711993 \n    2.793575 \n    1.2215939 \n    1.0000000 \n    4.611607 \n    4.955596 \n    3.042407 \n    3.354264 \n  \n  \n    201802 \n    2.831908 \n    2.884280 \n    1.2880070 \n    1.0760911 \n    4.000004 \n    4.753406 \n    3.019136 \n    3.139175 \n  \n  \n    201803 \n    2.752288 \n    2.908013 \n    1.2612020 \n    1.0000000 \n    4.000000 \n    4.732663 \n    3.064530 \n    3.133537 \n  \n  \n    201804 \n    2.980844 \n    2.969947 \n    1.3182645 \n    1.0141683 \n    4.752013 \n    5.075049 \n    3.055087 \n    3.526599 \n  \n  \n    201805 \n    2.980844 \n    2.961960 \n    1.2880070 \n    1.0000000 \n    5.272973 \n    5.075049 \n    3.065584 \n    3.610773 \n  \n  \n    201806 \n    2.980844 \n    3.000000 \n    1.2879126 \n    1.0968928 \n    5.195765 \n    5.433788 \n    3.061135 \n    3.801067 \n  \n  \n    201807 \n    2.982717 \n    2.883912 \n    1.1878090 \n    1.0497167 \n    4.917837 \n    5.207691 \n    3.094729 \n    3.711997 \n  \n  \n    201808 \n    3.000000 \n    3.000000 \n    1.2786471 \n    1.0000000 \n    5.075049 \n    5.162554 \n    3.068000 \n    3.196795 \n  \n  \n    201809 \n    2.995079 \n    3.000000 \n    1.3001620 \n    1.0000000 \n    5.022497 \n    5.323853 \n    3.113970 \n    3.087066 \n  \n  \n    201810 \n    3.000000 \n    3.000000 \n    1.2880070 \n    1.1975554 \n    5.075049 \n    5.272973 \n    3.036429 \n    3.465295 \n  \n  \n    201811 \n    2.969947 \n    2.883912 \n    1.1878090 \n    1.0000000 \n    5.075049 \n    5.000339 \n    3.056169 \n    3.058216 \n  \n  \n    201812 \n    3.000000 \n    2.980844 \n    1.3542874 \n    1.2046406 \n    5.098937 \n    5.000339 \n    3.033847 \n    3.076178 \n  \n  \n    201901 \n    2.969947 \n    2.969947 \n    1.2880070 \n    1.1906530 \n    5.143975 \n    5.306211 \n    3.060074 \n    3.143794 \n  \n  \n    201902 \n    2.793575 \n    2.768439 \n    1.1976033 \n    1.0572500 \n    4.381966 \n    4.700791 \n    3.014273 \n    3.045013 \n  \n  \n    201903 \n    2.816437 \n    2.864411 \n    1.2880070 \n    1.0133870 \n    4.583594 \n    4.450937 \n    3.002415 \n    3.080776 \n  \n  \n    201904 \n    2.598124 \n    2.693518 \n    1.1111110 \n    1.0000000 \n    4.035463 \n    4.546977 \n    3.009228 \n    3.039631 \n  \n  \n    201905 \n    2.453082 \n    2.593332 \n    1.0304210 \n    1.0000000 \n    3.999662 \n    4.583594 \n    2.987517 \n    3.028864 \n  \n  \n    201906 \n    2.670152 \n    2.662437 \n    1.0365318 \n    1.0000000 \n    4.872115 \n    4.891215 \n    3.029801 \n    3.081023 \n  \n  \n    201907 \n    2.590599 \n    2.579366 \n    1.0031077 \n    1.0000000 \n    4.534411 \n    4.937747 \n    2.939522 \n    3.022346 \n  \n  \n    201908 \n    2.414449 \n    2.501432 \n    1.0000000 \n    1.0000000 \n    4.374343 \n    4.828501 \n    2.912732 \n    2.983591 \n  \n  \n    201909 \n    2.483634 \n    2.374794 \n    1.0000000 \n    0.7932609 \n    4.140936 \n    4.320539 \n    2.968139 \n    3.005824 \n  \n  \n    201910 \n    2.330186 \n    2.378875 \n    1.0000000 \n    1.0000000 \n    3.819017 \n    4.450937 \n    2.975578 \n    3.019154 \n  \n  \n    201911 \n    2.347553 \n    2.521351 \n    1.0000000 \n    1.0000000 \n    4.000000 \n    4.348736 \n    2.958856 \n    3.011095 \n  \n  \n    201912 \n    2.527644 \n    2.537525 \n    1.0190988 \n    1.0000000 \n    4.500498 \n    4.906164 \n    2.998355 \n    3.033857 \n  \n  \n    202001 \n    2.495654 \n    2.537525 \n    1.0754316 \n    1.0000000 \n    4.319292 \n    4.642408 \n    2.978672 \n    3.074896 \n  \n  \n    202002 \n    2.537525 \n    2.588671 \n    1.1703846 \n    1.0000000 \n    4.068836 \n    4.381966 \n    2.968435 \n    3.148808 \n  \n  \n    202003 \n    2.537525 \n    2.399339 \n    0.2304544 \n    0.5961290 \n    5.310169 \n    5.011864 \n    2.976225 \n    3.006708 \n  \n  \n    202004 \n    2.623311 \n    2.623311 \n    0.0000000 \n    1.0000000 \n    6.000000 \n    5.657888 \n    3.015135 \n    3.149133 \n  \n  \n    202005 \n    3.000000 \n    2.610335 \n    0.4530818 \n    0.6104578 \n    6.414917 \n    5.730984 \n    4.064553 \n    3.322808 \n  \n  \n    202006 \n    2.681402 \n    2.473736 \n    0.7317622 \n    0.9928851 \n    5.688994 \n    5.086316 \n    3.411253 \n    3.065711 \n  \n  \n    202007 \n    2.887446 \n    2.731703 \n    0.8862122 \n    1.0000000 \n    6.000000 \n    5.500000 \n    3.144694 \n    3.259721 \n  \n  \n    202008 \n    3.000000 \n    2.980844 \n    1.0000000 \n    1.0000000 \n    6.000000 \n    5.760430 \n    3.803460 \n    3.180833 \n  \n  \n    202009 \n    2.980844 \n    2.742111 \n    1.0000000 \n    0.9135693 \n    5.874006 \n    5.344923 \n    3.441661 \n    3.251740 \n  \n  \n    202010 \n    2.836102 \n    2.702108 \n    1.0000000 \n    1.0000000 \n    6.000000 \n    5.191454 \n    3.416837 \n    3.437079 \n  \n  \n    202011 \n    2.962508 \n    2.819740 \n    1.0000000 \n    1.0000000 \n    5.944291 \n    5.774719 \n    3.494121 \n    3.376649 \n  \n  \n    202012 \n    3.000000 \n    2.980844 \n    1.2811244 \n    1.0000000 \n    6.000000 \n    5.904644 \n    4.307177 \n    3.199938 \n  \n  \n    202101 \n    3.046670 \n    3.028186 \n    1.5581499 \n    1.1878090 \n    6.754469 \n    6.000000 \n    4.548024 \n    4.016052 \n  \n  \n    202102 \n    3.090884 \n    3.000000 \n    1.5469180 \n    1.1869305 \n    6.146096 \n    6.000000 \n    4.578317 \n    4.374958 \n  \n  \n    202103 \n    3.244256 \n    3.092952 \n    1.8534898 \n    1.4624749 \n    6.280352 \n    6.297847 \n    4.920812 \n    4.674566 \n  \n  \n    202104 \n    3.360180 \n    3.098045 \n    1.9892329 \n    1.4624749 \n    6.235626 \n    6.310134 \n    4.620441 \n    4.307381 \n  \n  \n    202105 \n    4.000000 \n    3.569710 \n    2.3525007 \n    1.8115404 \n    8.200266 \n    6.645209 \n    5.261723 \n    4.963713 \n  \n  \n    202106 \n    4.801745 \n    3.546918 \n    2.4637444 \n    1.3498780 \n    8.529976 \n    6.973494 \n    5.481582 \n    4.751600 \n  \n  \n    202107 \n    4.842451 \n    3.711669 \n    2.5060110 \n    1.5811223 \n    8.452378 \n    8.000000 \n    5.410090 \n    4.857979 \n  \n  \n    202108 \n    5.179827 \n    3.999662 \n    2.7170422 \n    1.3798504 \n    8.729171 \n    7.763283 \n    5.816178 \n    4.947668 \n  \n  \n    202109 \n    5.310169 \n    4.193985 \n    2.6700342 \n    1.7182721 \n    8.713920 \n    7.656233 \n    5.522046 \n    4.965563 \n  \n  \n    202110 \n    5.653467 \n    4.211177 \n    2.6414132 \n    1.4647722 \n    9.265794 \n    8.385000 \n    6.816117 \n    5.031971 \n  \n  \n    202111 \n    6.000000 \n    4.011200 \n    3.0154593 \n    1.0130244 \n    9.733616 \n    8.000000 \n    7.233366 \n    5.177737 \n  \n  \n    202112 \n    5.990274 \n    4.000000 \n    3.0000000 \n    1.0285025 \n    9.283830 \n    7.785626 \n    7.025210 \n    4.854648 \n  \n  \n    202201 \n    5.788440 \n    3.477248 \n    2.6233110 \n    0.1733402 \n    9.298556 \n    6.793561 \n    7.118849 \n    4.476177 \n  \n  \n    202202 \n    6.000000 \n    3.774802 \n    3.0095823 \n    0.5622389 \n    9.661465 \n    7.141932 \n    7.452888 \n    4.724739 \n  \n  \n    202203 \n    6.583898 \n    3.674923 \n    3.2615321 \n    0.0455311 \n    10.000000 \n    7.865997 \n    8.462144 \n    4.882196 \n  \n  \n    202204 \n    6.345438 \n    3.902376 \n    3.5794370 \n    0.0378796 \n    9.745214 \n    7.899619 \n    8.128750 \n    4.699248 \n  \n  \n    202205 \n    6.576015 \n    3.879077 \n    3.5757761 \n    0.0000000 \n    10.000000 \n    8.000000 \n    8.366904 \n    4.756304 \n  \n  \n    202206 \n    6.775855 \n    3.618485 \n    3.5469179 \n    0.0000000 \n    10.124187 \n    8.043677 \n    8.426487 \n    4.597435 \n  \n  \n    202207 \n    6.223626 \n    3.182451 \n    3.0000000 \n    0.0000000 \n    10.000000 \n    6.781112 \n    8.175066 \n    3.870215 \n  \n  \n    202208 \n    5.745714 \n    2.755268 \n    2.0113456 \n    -0.0653624 \n    9.283830 \n    6.117932 \n    7.098406 \n    3.730435 \n  \n  \n    202209 \n    5.440794 \n    2.909971 \n    1.8285562 \n    -0.0138081 \n    9.101035 \n    6.000000 \n    6.982421 \n    3.318044 \n  \n  \n    202210 \n    5.944291 \n    3.110663 \n    2.4414747 \n    0.0000000 \n    9.573191 \n    6.366208 \n    7.553532 \n    3.984792 \n  \n\n\n\n\n\nNow let me format the dates properly to treat these data as a time series.\n\nInflation.Expectations %<>%\n  rename(date = 1) %>% \n  mutate(date = yearmonth(parse_date_time(date, orders = \"%Y%m\")))\n\nThat’s pretty much all that is required to visualize them. Let’s have a look. I will use the fpp3 library and autoplot().\n\nlibrary(fpp3)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tsibbledata 0.4.1     ✔ fable       0.3.2\n✔ feasts      0.3.0     \n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()        masks base::date()\n✖ magrittr::extract()      masks tidyr::extract()\n✖ dplyr::filter()          masks stats::filter()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ tsibble::intersect()     masks base::intersect()\n✖ tsibble::interval()      masks lubridate::interval()\n✖ dplyr::lag()             masks stats::lag()\n✖ magrittr::set_names()    masks purrr::set_names()\n✖ tsibble::setdiff()       masks base::setdiff()\n✖ tsibble::union()         masks base::union()\n\nInflation.Expectations %>% \n  as_tsibble(index=date) %>% \n  autoplot(`Median three-year ahead expected inflation rate`) + \n  geom_line(aes(y=Inflation.Expectations$`Median one-year ahead expected inflation rate`, color=\"red\")) +\n  hrbrthemes::theme_ipsum() + guides(color = \"none\")\n\n\n\n\nNow let me pretty the plot.\n\nInflation.Expectations %>% \n  select(1:3) %>% \n  pivot_longer(c(2:3)) %>%\n  mutate(Variable = name) %>%\n  ggplot(aes(x=date, y=value, color=Variable)) +\n  geom_line(size=2) +\n  scale_color_viridis_d(option=\"C\") +\n  hrbrthemes::theme_ft_rc() +\n  theme(legend.position = \"bottom\") +\n  labs(y=\"Inflation Expectations\", \n       color=\"\",\n       title=\"New York Fed Inflation Expectations\",\n       caption = \"data: https://www.newyorkfed.org/medialibrary/interactives/sce/sce/downloads/data/frbny-sce-data.xlsx\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html",
    "href": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html",
    "title": "National Weather Service Portland, Part II",
    "section": "",
    "text": "I first will try to load it without any intervention to see what it looks like. As we will see, it is quite messy in a few easy ways and a few that are a bit more tricky.\n\nNWS <- read.csv(url(\"https://www.weather.gov/source/pqr/climate/webdata/Portland_dailyclimatedata.csv\"))\nhead(NWS, 10) %>% kable() %>%\n    kable_styling() %>%\n    scroll_box(width = \"100%\", height = \"500px\")\n\n\n\n \n  \n    Daily.Temperature.and.Precipitation.Data \n    X \n    X.1 \n    X.2 \n    X.3 \n    X.4 \n    X.5 \n    X.6 \n    X.7 \n    X.8 \n    X.9 \n    X.10 \n    X.11 \n    X.12 \n    X.13 \n    X.14 \n    X.15 \n    X.16 \n    X.17 \n    X.18 \n    X.19 \n    X.20 \n    X.21 \n    X.22 \n    X.23 \n    X.24 \n    X.25 \n    X.26 \n    X.27 \n    X.28 \n    X.29 \n    X.30 \n    X.31 \n    X.32 \n    X.33 \n  \n \n\n  \n    Portland, Oregon Airport \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n  \n    Data Starts: \n     \n     \n    13 Oct 1940 \n     \n     \n     \n    END Year: \n     \n    2019 \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n  \n     \n    TX is Maximum Temperature (deg F), TX is Minimum Temperature (deg F), PR is Precipitation (inches), SN is Snowfall  (inches) \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n  \n     \n    Example:   High Temperature  23 October 1940 is 58 while low was 53 deg. \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n  \n    YR \n    MO \n     \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n    7 \n    8 \n    9 \n    10 \n    11 \n    12 \n    13 \n    14 \n    15 \n    16 \n    17 \n    18 \n    19 \n    20 \n    21 \n    22 \n    23 \n    24 \n    25 \n    26 \n    27 \n    28 \n    29 \n    30 \n    31 \n    AVG or Total \n  \n  \n    1940 \n    10 \n    TX \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    75 \n    70 \n    64 \n    72 \n    72 \n    78 \n    78 \n    64 \n    63 \n    61 \n    58 \n    57 \n    57 \n    57 \n    56 \n    53 \n    59 \n    59 \n    52 \n    M \n  \n  \n    1940 \n    10 \n    TN \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    57 \n    53 \n    52 \n    50 \n    58 \n    58 \n    59 \n    54 \n    48 \n    41 \n    53 \n    48 \n    41 \n    38 \n    37 \n    45 \n    48 \n    50 \n    46 \n    M \n  \n  \n    1940 \n    10 \n    PR \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    0.01 \n    T \n    T \n    0 \n    0.13 \n    0 \n    T \n    0.14 \n    0.05 \n    0 \n    0.63 \n    1.03 \n    0 \n    0 \n    T \n    0.18 \n    0.58 \n    0.5 \n    0.25 \n    M \n  \n  \n    1940 \n    10 \n    SN \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    M \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\nThe column names are stored in the seventh row; to properly import this. In addition, there are two missing value codes: M and - that will have to be accounted for. I will use skip to skip the first 6 rows and declare two distinct values to be encoded as missing. Let’s see what we get.\n\nNWS <- read.csv(url(\"https://www.weather.gov/source/pqr/climate/webdata/Portland_dailyclimatedata.csv\"), skip=6, na.strings = c(\"M\",\"-\"))\nhead(NWS, 10)\n\n     YR MO  X   X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  X11  X12  X13\n1  1940 10 TX <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>   75\n2  1940 10 TN <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>   57\n3  1940 10 PR <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> 0.01\n4  1940 10 SN <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>    0\n5  1940 11 TX   52   53   47   55   51   58   56   50   48   47   46   45   45\n6  1940 11 TN   40   38   36   32   42   46   46   42   35   34   35   33   34\n7  1940 11 PR 0.17 0.02    T    0 0.07 0.28 0.85 0.29 0.02 0.01 0.01    0    0\n8  1940 11 SN    0    0    0    0    0    0    0    0    0    0    0    0    0\n9  1940 12 TX   51   53   52   51   56   54   50   51   48   50   46   45   43\n10 1940 12 TN   42   40   42   42   44   37   34   35   32   26   34   28   27\n   X14 X15 X16  X17  X18 X19  X20  X21 X22  X23  X24 X25 X26  X27  X28  X29 X30\n1   70  64  72   72   78  78   64   63  61   58   57  57  57   56   53   59  59\n2   53  52  50   58   58  59   54   48  41   53   48  41  38   37   45   48  50\n3    T   T   0 0.13    0   T 0.14 0.05   0 0.63 1.03   0   0    T 0.18 0.58 0.5\n4    0   0   0    0    0   0    0    0   0    0    0   0   0    0    0    0   0\n5   47  53  49   46   49  46   49   50  44   42   44  51  44   45   59   57  45\n6   33  28  27   36   30  29   36   33  28   37   35  37  36   38   43   40  39\n7    0   0   0 0.29 0.01   0 0.37    T   0 0.12 0.62   0   0 0.51 0.89    T   T\n8    0   0   0    0    0   0    0    0   0    0    0   0   0    0    0    0   0\n9   40  39  39   41   41  45   46   62  60   56   53  54  45   50   51   43  44\n10  25  29  33   35   34  35   41   39  39   42   42  42  40   38   36   35  37\n    X31 AVG.or.Total\n1    52         <NA>\n2    46         <NA>\n3  0.25         <NA>\n4     0            0\n5  <NA>         49.1\n6  <NA>         35.9\n7  <NA>         4.53\n8  <NA>            0\n9    45         48.5\n10   32           36\n\n\nTwo other things are of note. The first one is that R really doesn’t like columns to be named as numbers so we have an X in front of the numeric days. The second is that the column denoting which variable the rows represent is now X. Let me rename X to be Variable.\n\nNWS <- read.csv(url(\"https://www.weather.gov/source/pqr/climate/webdata/Portland_dailyclimatedata.csv\"), skip=6, na.strings = c(\"M\",\"-\")) %>% \n  rename(Variable = X)\nstr(NWS)\n\n'data.frame':   3808 obs. of  35 variables:\n $ YR          : int  1940 1940 1940 1940 1940 1940 1940 1940 1940 1940 ...\n $ MO          : int  10 10 10 10 11 11 11 11 12 12 ...\n $ Variable    : chr  \"TX\" \"TN\" \"PR\" \"SN\" ...\n $ X1          : chr  NA NA NA NA ...\n $ X2          : chr  NA NA NA NA ...\n $ X3          : chr  NA NA NA NA ...\n $ X4          : chr  NA NA NA NA ...\n $ X5          : chr  NA NA NA NA ...\n $ X6          : chr  NA NA NA NA ...\n $ X7          : chr  NA NA NA NA ...\n $ X8          : chr  NA NA NA NA ...\n $ X9          : chr  NA NA NA NA ...\n $ X10         : chr  NA NA NA NA ...\n $ X11         : chr  NA NA NA NA ...\n $ X12         : chr  NA NA NA NA ...\n $ X13         : chr  \"75\" \"57\" \"0.01\" \"0\" ...\n $ X14         : chr  \"70\" \"53\" \"T\" \"0\" ...\n $ X15         : chr  \"64\" \"52\" \"T\" \"0\" ...\n $ X16         : chr  \"72\" \"50\" \"0\" \"0\" ...\n $ X17         : chr  \"72\" \"58\" \"0.13\" \"0\" ...\n $ X18         : chr  \"78\" \"58\" \"0\" \"0\" ...\n $ X19         : chr  \"78\" \"59\" \"T\" \"0\" ...\n $ X20         : chr  \"64\" \"54\" \"0.14\" \"0\" ...\n $ X21         : chr  \"63\" \"48\" \"0.05\" \"0\" ...\n $ X22         : chr  \"61\" \"41\" \"0\" \"0\" ...\n $ X23         : chr  \"58\" \"53\" \"0.63\" \"0\" ...\n $ X24         : chr  \"57\" \"48\" \"1.03\" \"0\" ...\n $ X25         : chr  \"57\" \"41\" \"0\" \"0\" ...\n $ X26         : chr  \"57\" \"38\" \"0\" \"0\" ...\n $ X27         : chr  \"56\" \"37\" \"T\" \"0\" ...\n $ X28         : chr  \"53\" \"45\" \"0.18\" \"0\" ...\n $ X29         : chr  \"59\" \"48\" \"0.58\" \"0\" ...\n $ X30         : chr  \"59\" \"50\" \"0.5\" \"0\" ...\n $ X31         : chr  \"52\" \"46\" \"0.25\" \"0\" ...\n $ AVG.or.Total: chr  NA NA NA \"0\" ...\n\n\nIt is disappointing that everything is stored as character type. That will prove advantageous in one respect because there is some /A garbage embedded in two of the variables (SN and PR). Here, I will ask R to find all columns that are stored as character and ask it to remove the string.\n\nNWS <- NWS %>% mutate(across(where(is.character), ~str_remove(.x, \"/A\")))\n\nNow, we will have to fix the values T in the precipitation and snow variables [which are currently stored in repeated rows]. Nevertheless, this should give me what I need to create the monthly data.\n\n\nThe daily data will necessarily not involve the column of Totals/Averages that we used for the monthly data so let us eliminate it.\n\nNWS.Daily <- NWS %>% select(-AVG.or.Total)\n\nNow I want to rename the columns with names X1, X2, …, X31 to Day.1 to Day.31 for clarity. It is largely inconsequential, it would work on the X’s but I prefer nicely labeled intermediate steps.\n\nnames(NWS.Daily) <- c(\"YR\",\"MO\",\"Variable\",paste0(\"Day.\",1:31))\n\nThe next step is to tidy the data. First, let me use pivot_longer on every column that starts with Day. putting the variable names in Day and variable values in value.\n\nNWS.Daily.Base <- NWS.Daily %>% \n  pivot_longer(., cols=starts_with(\"Day.\"), names_to = \"Day\", values_to = \"value\")\nhead(NWS.Daily.Base)\n\n# A tibble: 6 × 5\n     YR    MO Variable Day   value\n  <int> <int> <chr>    <chr> <chr>\n1  1940    10 TX       Day.1 <NA> \n2  1940    10 TX       Day.2 <NA> \n3  1940    10 TX       Day.3 <NA> \n4  1940    10 TX       Day.4 <NA> \n5  1940    10 TX       Day.5 <NA> \n6  1940    10 TX       Day.6 <NA> \n\n\nNow I want to turn the days into numbers [they are character above] and then use pivot_wider to get the four variables into unique columns, recode trace [T] where they exist to numbers that are half the size of the smallest values, turn them into numbers, and create a date.\n\nNWS.Daily.Base %<>%  mutate(Day = str_remove(Day, \"Day.\")) %>%  \n  pivot_wider(., names_from = \"Variable\", values_from = \"value\") \nNWS.Daily <- NWS.Daily.Base %>% mutate(PR = recode(PR, T = \"O.005\"), SN = recode(SN, T = \"O.005\")) %>% \n  mutate(TX = as.numeric(TX), TN = as.numeric(TN), PR = as.numeric(PR), SN = as.numeric(SN), \n         date = as.Date(paste(MO,Day,YR,sep=\"-\"), format=\"%m-%d-%Y\")\n         )\nNWS.Daily.Clean <- NWS.Daily %>% filter(!is.na(date))\nhead(NWS.Daily.Clean)\n\n# A tibble: 6 × 8\n     YR    MO Day      TX    TN    PR    SN date      \n  <int> <int> <chr> <dbl> <dbl> <dbl> <dbl> <date>    \n1  1940    10 1        NA    NA    NA    NA 1940-10-01\n2  1940    10 2        NA    NA    NA    NA 1940-10-02\n3  1940    10 3        NA    NA    NA    NA 1940-10-03\n4  1940    10 4        NA    NA    NA    NA 1940-10-04\n5  1940    10 5        NA    NA    NA    NA 1940-10-05\n6  1940    10 6        NA    NA    NA    NA 1940-10-06\n\n\nThis is exactly what I needed."
  },
  {
    "objectID": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#time-series-plots",
    "href": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#time-series-plots",
    "title": "National Weather Service Portland, Part II",
    "section": "Time Series Plots",
    "text": "Time Series Plots\n\nDaily\n\nNWS.Daily.Clean <- NWS.Daily.Clean %>% as_tsibble(., index=date)\nNWS.Daily.Clean %>% filter(date > \"2010-01-01\") %>% autoplot(TX) + labs(title=\"Daily High Temperatures in Portland, Oregon\", caption = \"Data from NWS\", x = \"Date\", y=\"High Temperature (deg F)\")"
  },
  {
    "objectID": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#seasonal-plots",
    "href": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#seasonal-plots",
    "title": "National Weather Service Portland, Part II",
    "section": "Seasonal Plots",
    "text": "Seasonal Plots\n\nDaily\n\nNWS.Daily.Clean %>% filter(date > \"2015-01-01\") %>% gg_season(TX, labels = \"both\")"
  },
  {
    "objectID": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#subseries-plots",
    "href": "posts/2022-03-19-national-weather-service-portland-part-ii/index.html#subseries-plots",
    "title": "National Weather Service Portland, Part II",
    "section": "Subseries Plots",
    "text": "Subseries Plots\n\nDaily\nDaily subseries plots are a mess because there are 31 and I am not sure that there would be much instructive anyway as daily variation in temperature is quite noisy.\n\nNWS.Daily.Clean %>% gg_subseries(TX, period=\"weeks\")"
  },
  {
    "objectID": "posts/2022-04-14-flows-to-marion-county/index.html",
    "href": "posts/2022-04-14-flows-to-marion-county/index.html",
    "title": "Flows to Marion County",
    "section": "",
    "text": "The tidycensus package is really neat. There is an example in the vignettes that tracks flows of people by county that I wanted to recreate for Marion County Oregon. It is the vignette on other datasets.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nmarion_flows <- get_flows(\n  geography = \"county\",\n  state = \"OR\",\n  county = \"Marion\",\n  year = 2018,\n  geometry = TRUE\n  )\n\nI want to directly as I can borrow this.\n\ntop_move_in <- marion_flows %>% \n  filter(!is.na(GEOID2), variable == \"MOVEDIN\") %>% \n  slice_max(n = 25, order_by = estimate) %>% \n  mutate(\n    width = log(estimate),\n    tooltip = paste0(\n      scales::comma(estimate * 5, 1),\n      \" people moved from \", str_remove(FULL2_NAME, \"Metro Area\"),\n      \" to \", str_remove(FULL1_NAME, \"Metro Area\"), \" between 2014 and 2018\"\n      )\n    )\n\n\nlibrary(htmlwidgets)\n# md <- \ntop_move_in %>% \n  mapdeck(pitch = 45) %>% \n  add_arc(\n    origin = \"centroid1\",\n    destination = \"centroid2\",\n    stroke_width = \"width\",\n    auto_highlight = TRUE,\n    highlight_colour = \"#8c43facc\",\n    tooltip = \"tooltip\"\n  )\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite\n\n\n\n\n\n#frameWidget(md)"
  },
  {
    "objectID": "posts/2022-04-20-mapping-voters/index.html",
    "href": "posts/2022-04-20-mapping-voters/index.html",
    "title": "Mapping Voters",
    "section": "",
    "text": "Robert Husseman @RHusseman on Twitter is running to represent Oregon District 21 in the Oregon House of Representatives. He has a platform that focuses on housing and livability in the state by making sensible allocations of public resources to public problems. He has an MBA; he was my student and his head and heart are very much in the right place to serve Oregon well.\nHis campaign has a voter list for the Democratic voters in Oregon 21. He shared access to this. Let me load them up from a saved Excel spreadsheet derived from a Google sheet."
  },
  {
    "objectID": "posts/2022-04-20-mapping-voters/index.html#geocoding",
    "href": "posts/2022-04-20-mapping-voters/index.html#geocoding",
    "title": "Mapping Voters",
    "section": "Geocoding",
    "text": "Geocoding\nI ran across a package called tidygeocoder to do the heavy lifting. Here is what I have to work with.\n\nlibrary(tidygeocoder)\nnames(RHCamp)\n\n [1] \"VOTER_ID\"          \"FIRST_NAME\"        \"MIDDLE_NAME\"      \n [4] \"LAST_NAME\"         \"NAME_SUFFIX\"       \"BIRTH_DATE\"       \n [7] \"CONFIDENTIAL\"      \"EFF_REGN_DATE\"     \"STATUS\"           \n[10] \"PARTY_CODE\"        \"PHONE_NUM\"         \"UNLISTED\"         \n[13] \"COUNTY\"            \"RES_ADDRESS_1\"     \"RES_ADDRESS_2\"    \n[16] \"HOUSE_NUM\"         \"HOUSE_SUFFIX\"      \"PRE_DIRECTION\"    \n[19] \"STREET_NAME\"       \"STREET_TYPE\"       \"POST_DIRECTION\"   \n[22] \"UNIT_TYPE\"         \"UNIT_NUM\"          \"ADDR_NON_STD\"     \n[25] \"CITY\"              \"STATE\"             \"ZIP_CODE\"         \n[28] \"ZIP_PLUS_FOUR\"     \"EFF_ADDRESS_1\"     \"EFF_ADDRESS_2\"    \n[31] \"EFF_ADDRESS_3\"     \"EFF_ADDRESS_4\"     \"EFF_CITY\"         \n[34] \"EFF_STATE\"         \"EFF_ZIP_CODE\"      \"EFF_ZIP_PLUS_FOUR\"\n[37] \"ABSENTEE_TYPE\"     \"PRECINCT_NAME\"     \"PRECINCT\"         \n[40] \"SPLIT\""
  },
  {
    "objectID": "posts/2022-04-20-mapping-voters/index.html#a-great-thing-about-r",
    "href": "posts/2022-04-20-mapping-voters/index.html#a-great-thing-about-r",
    "title": "Mapping Voters",
    "section": "A Great Thing about R",
    "text": "A Great Thing about R\nThere is a vignette explaining the usage.\nMy use case is pretty easy though there is a lot of data. I want to try this with just the addresses. I have loaded the tidyverse and now I want to peel off just the data that I need in order to geocode it, and then process it with the Census geocoder. This results in 9443 queries.\n\nGeoMe <- RHCamp %>% select(VOTER_ID, RES_ADDRESS_1, CITY, STATE) %>% geocode(street = RES_ADDRESS_1, city=CITY, state=STATE, method=\"census\")\nNewData <- GeoMe %>% left_join(., RHCamp)\nsave(NewData, file=\"~/Desktop/NewDataRH.RData\")\n\nThis takes quite a long time to run – 667.1 seconds.\nWhen it is complete, I will want to figure out how to map them in a way that is best fit for assisting a campaign. This seems like a good use for leaflet. Rather than taking 10 minutes or so to compile each time, I saved them off. I am loading them from a local file in the first line."
  },
  {
    "objectID": "posts/2022-04-20-mapping-voters/index.html#leaflet-mapping",
    "href": "posts/2022-04-20-mapping-voters/index.html#leaflet-mapping",
    "title": "Mapping Voters",
    "section": "Leaflet Mapping",
    "text": "Leaflet Mapping\n\nload(\"data/NewDataRH.RData\")\n\nThe leaflet package uses very powerful mapping tools that will be perfect for this use case because the maps have easy zoom features.\n\nlibrary(leaflet)\nNewData %>% mutate(labMy = paste0(FIRST_NAME,LAST_NAME,RES_ADDRESS_1,sep=\"\\n\")) %>%\n  leaflet(data = .) %>% \n  addTiles() %>%\n  addCircleMarkers(~long, ~lat, label = ~labMy, fillOpacity = 0.1, stroke=FALSE, radius=1.6)\n\nWarning in validateCoords(lng, lat, funcName): Data contains 641 rows with\neither missing or invalid lat/lon values and will be ignored\n\n\n\n\n\n\nThat works but it isn’t quite right. Turns out that I will have to mess with html to get the labels quite right.\nSo, of course, I went to stackoverflow."
  },
  {
    "objectID": "posts/2022-04-20-mapping-voters/index.html#building-better-labels",
    "href": "posts/2022-04-20-mapping-voters/index.html#building-better-labels",
    "title": "Mapping Voters",
    "section": "Building Better Labels",
    "text": "Building Better Labels\nI found a post on how to do this.\nThe key piece of code seems to be:\n\nlabs <- lapply(seq(nrow(NewData)), function(i) {\n  paste0( '<p>', NewData[i,\"RES_ADDRESS_1\"],'</p>' )\n})\nNewData %>%\n  leaflet(data = .) %>% \n  addTiles() %>%\n  addCircleMarkers(~long, ~lat, label = lapply(labs, htmltools::HTML), fillOpacity = 0.15, stroke=FALSE, radius=1.6)\n\nWarning in validateCoords(lng, lat, funcName): Data contains 641 rows with\neither missing or invalid lat/lon values and will be ignored"
  },
  {
    "objectID": "posts/2022-04-22-a-pigeon-becomes-a-barplot/index.html",
    "href": "posts/2022-04-22-a-pigeon-becomes-a-barplot/index.html",
    "title": "A Pigeon Becomes a Barplot",
    "section": "",
    "text": "It really is an amazing pigeon. Apologies that I haven’t a photo credit for it; I found it one day on twitter as it went viral. It is a New York City pigeon.\n\n\n\nPigeon\n\n\nI want to color something with colors from that pigeon. It has an amazing array of colors.\nTLDR: Extract colors from pigeon photo to palette. Then use palette in ggplot."
  },
  {
    "objectID": "posts/2022-04-22-a-pigeon-becomes-a-barplot/index.html#the-palette",
    "href": "posts/2022-04-22-a-pigeon-becomes-a-barplot/index.html#the-palette",
    "title": "A Pigeon Becomes a Barplot",
    "section": "The Palette",
    "text": "The Palette\nFirst, I have to get the color palette.\nR has a package called imgpalr that will extract colors from an image. If I need to, it is install.packages(\"imgpalr\").\nIn the following code chunk, I will load the package and then I want to point it to the pigeon image that I downloaded. In my case, it is in the same directory as the R Markdown file and is called Pigeon.png.\nInside the command image_pal(), I point it to an image, I specify how many colors [5], what type of color scheme, I want qualitative, and then some characteristics of the colors. The plot will show me the image alongside the palette though it is upside down.\n\nlibrary(imgpalr)\nPigeon.colors <- image_pal(\"~/Downloads/Pigeon.png\", # This will need to be adjusted to the actual file location on your computer.  Mine is in my downloads.\n          n = 5, # How many colors?\n          type = \"qual\", # Type of palette?\n          saturation = c(0.75, 1), \n          brightness = c(0.75, 1), \n          plot = TRUE, # Show the image and the palette?\n          bw = c(0.7, 0.95)\n          )\n\n\n\nPigeon.colors\n\n[1] \"#A3A1E6\" \"#D5A0F0\" \"#E0C699\" \"#F495E3\" \"#97BCE2\"\n\n\nNow I have a palette in the markdown environment called Pigeon.colors. Now I need some data; I want five categories to match my palette. The rest is junk.\n\nJunk.data <- data.frame(Stuff=c(\"Hi\",\"Howdy\",\"Hello\",\"Hola\",\"Hallo\"), vals = runif(5, 0, 1))\nJunk.data\n\n  Stuff       vals\n1    Hi 0.80980408\n2 Howdy 0.02055467\n3 Hello 0.11078097\n4  Hola 0.56221227\n5 Hallo 0.79464218\n\n\nI want to graph it using a barplot equivalent because I have the height of the bars in vals. Let me use the fill aesthetic to fill the bars in by the five discrete things stored as Stuff. The trick to using it is to manually specify the colors and point it to the Pigeon.colors above.\n\nlibrary(ggplot2)\nggplot(Junk.data) + \n  aes(x=Stuff, y=vals, fill=Stuff) + \n  geom_col() + \n  scale_fill_manual(values = Pigeon.colors)"
  },
  {
    "objectID": "posts/2022-04-22-flows-to-hidalgo-county/index.html",
    "href": "posts/2022-04-22-flows-to-hidalgo-county/index.html",
    "title": "Flows to Hidalgo County",
    "section": "",
    "text": "The tidycensus package is really neat. There is an example in the vignettes that tracks flows of people by county that I wanted to recreate for Hidalgo County Texas. The key information on how to do it is in the vignette on other datasets.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nhidalgo_flows <- get_flows(\n  geography = \"county\",\n  state = \"TX\",\n  county = \"Hidalgo\",\n  year = 2018,\n  geometry = TRUE\n  )\n\nFirst, to grab the flows of people.\n\ntop_move_in <- hidalgo_flows %>% \n  filter(!is.na(GEOID2), variable == \"MOVEDIN\") %>% \n  slice_max(n = 25, order_by = estimate) %>% \n  mutate(\n    width = log(estimate),\n    tooltip = paste0(\n      scales::comma(estimate * 5, 1),\n      \" people moved from \", FULL2_NAME,\n      \" to \", FULL1_NAME, \" between 2014 and 2018\"\n      )\n    )\n\n\nlibrary(htmlwidgets)\n# md <- \ntop_move_in %>% \n  mapdeck(pitch = 45) %>% \n  add_arc(\n    origin = \"centroid1\",\n    destination = \"centroid2\",\n    stroke_width = \"width\",\n    auto_highlight = TRUE,\n    highlight_colour = \"#8c43facc\",\n    tooltip = \"tooltip\"\n  )\n\nRegistered S3 method overwritten by 'jsonify':\n  method     from    \n  print.json jsonlite"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]